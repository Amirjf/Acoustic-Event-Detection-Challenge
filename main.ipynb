{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_curve, auc, make_scorer, roc_auc_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from data_utils import load_metadata, sample_sounds_by_category, load_wave_data, load_all_sounds,load_paths_from_config\n",
    "from feature_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_features=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path, audio_files_path = load_paths_from_config()\n",
    "\n",
    "\n",
    "# 1.1) check if the paths exist\n",
    "assert os.path.exists(csv_file_path), \"CSV file not found!\"\n",
    "assert os.path.exists(audio_files_path), \"Audio directory not found!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load metadata\n",
    "df = load_metadata(csv_file_path)\n",
    "\n",
    "print(df)\n",
    "print(\"Missing values:\\n\", df.isnull().sum()) # Check for missing values\n",
    "print(df.describe()) # Show summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Define categories\n",
    "categories = {\n",
    "        'Animals': ['dog', 'rooster'],\n",
    "        'Natural soundscapes & water sounds': ['thunderstorm', 'pouring_water'],\n",
    "        'Human sounds': ['snoring', 'sneezing'],\n",
    "        'Interior/domestic sounds': ['clock_alarm', 'vacuum_cleaner'],\n",
    "        'Exterior/urban noises': ['siren', 'helicopter']\n",
    "    }\n",
    "\n",
    "# Filter the dataframe to only include selected categories\n",
    "selected_classes = sum(categories.values(), [])  # Flatten the dictionary into a list of class names\n",
    "df_filtered = df[df['category'].isin(selected_classes)]  # Keep only selected categories\n",
    "\n",
    "class_counts = df_filtered['category'].value_counts()\n",
    "print(\"Class counts for selected categories:\\n\", class_counts)\n",
    "if all(class_counts == 40):\n",
    "    print(\"‚úÖ All classes have 40 rows.\")\n",
    "else:\n",
    "    print(\"‚ùå Some classes do not have 40 rows.\")\n",
    "    print(\"Classes with insufficient rows:\\n\", class_counts[class_counts != 40])\n",
    "\n",
    "# # Plot class distribution of selected categories\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# sns.countplot(x=df_filtered['category'])\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.title(\"Class Distribution (Selected Categories)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping dictionary from category to its target number\n",
    "category_to_target = df_filtered.set_index('category')['target'].to_dict()\n",
    "mapping_df = pd.DataFrame(category_to_target.items(), columns=['Category', 'Target'])\n",
    "print(mapping_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Sample sounds\n",
    "sampled_sounds = sample_sounds_by_category(df, categories)\n",
    "\n",
    "# Track missing and represented classes\n",
    "missing_classes = {}\n",
    "represented_classes = {}\n",
    "\n",
    "for category, expected_classes in categories.items():\n",
    "    # Extract actual classes from sampled sounds\n",
    "    sampled_classes = set(df[df['filename'].isin(sampled_sounds[category])]['category'])\n",
    "    \n",
    "    # Check which classes are missing\n",
    "    missing = set(expected_classes) - sampled_classes\n",
    "    represented_classes[category] = sampled_classes\n",
    "\n",
    "    if missing:\n",
    "        missing_classes[category] = missing\n",
    "\n",
    "    print(f\"Category: {category}, Sampled Classes: {sampled_classes}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n‚úÖ Represented Classes:\")\n",
    "for category, classes in represented_classes.items():\n",
    "    print(f\"- {category}: {classes}\")\n",
    "\n",
    "if missing_classes:\n",
    "    print(\"\\n‚ùå Missing Classes:\")\n",
    "    for category, classes in missing_classes.items():\n",
    "        print(f\"- {category}: {classes}\")\n",
    "else:\n",
    "    print(\"\\nüéâ All expected classes are present!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Load wave data\n",
    "wave_list_data_sampled = load_wave_data(sampled_sounds, audio_files_path) #load_sample\n",
    "wave_list_data = load_all_sounds(df, categories, audio_files_path) #load all sounds\n",
    "\n",
    "print(\"Total samples loaded:\", len(wave_list_data))\n",
    "print(\"Example sample rate:\", wave_list_data[0][2])\n",
    "print(\"Example waveform shape:\", wave_list_data[0][3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Extract all features\n",
    "# keys_list, combined_features = compute_combined_features_for_wave_list(wave_list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 8) Save all features\n",
    "\n",
    "if compute_features:\n",
    "    # 7) Extract selected features\n",
    "    # keys_list, mfcc_list, hist_list, spectral_list, zcr_list, envelope_list, hnr_list = compute_features_for_wave_list(wave_list_data)\n",
    "    keys_list, mfcc_list, hist_list, spectral_centroid_list, spectral_contrast_list, pitch_features_list, zcr_list, envelope_list, hnr_list = compute_features_for_wave_list(wave_list_data)\n",
    "\n",
    "    # 8) Save selected features\n",
    "    save_multiple_features_to_npz(\n",
    "        keys_list, \n",
    "        mfcc_list, \n",
    "        hist_list, \n",
    "        spectral_centroid_list,\n",
    "        spectral_contrast_list,\n",
    "        pitch_features_list, \n",
    "        zcr_list, \n",
    "        envelope_list, \n",
    "        hnr_list, \n",
    "        out_file=\"features/extracted_features_multiple_test.npz\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved features\n",
    "loaded_data = np.load(\"features/extracted_features_multiple_test.npz\")\n",
    "\n",
    "# Define which features to include (set True to include, False to exclude)\n",
    "feature_selection = {\n",
    "    'mfcc': True,\n",
    "    'hist': True,\n",
    "    'spectral_centroid': True,\n",
    "    'spectral_contrast': True,\n",
    "    'pitch_features': True,\n",
    "    'zcr': True,\n",
    "    'envelope': True,\n",
    "    'hnr': True\n",
    "}\n",
    "\n",
    "# Display the shapes of each feature type\n",
    "print(\"Feature shapes and selection status:\")\n",
    "for feature, include in feature_selection.items():\n",
    "    shape = loaded_data[feature].shape\n",
    "    status = \"‚úÖ\" if include else \"‚ùå\"\n",
    "    print(f\"{feature.capitalize()} shape: {shape} - Status: {status}\")\n",
    "\n",
    "# Extract features and labels\n",
    "keys_list = loaded_data['keys']\n",
    "mfcc_features = loaded_data['mfcc']\n",
    "hist_features = loaded_data['hist']\n",
    "spectral_centroid_features = loaded_data['spectral_centroid']\n",
    "spectral_contrast_features = loaded_data['spectral_contrast']\n",
    "pitch_features = loaded_data['pitch_features']\n",
    "zcr_features = loaded_data['zcr']\n",
    "envelope_features = loaded_data['envelope']\n",
    "hnr_features = loaded_data['hnr']\n",
    "\n",
    "\n",
    "# #Display first feature vectors for verification\n",
    "# print(\"First MFCC feature vector:\", loaded_data['mfcc'][0])\n",
    "# print(\"First histogram feature vector:\", loaded_data['hist'][0])\n",
    "# print(\"First spectral centroid feature vector:\", loaded_data['spectral_centroid'][0])\n",
    "# print(\"First spectral contrast feature vector:\", loaded_data['spectral_contrast'][0])\n",
    "# print(\"First pitch feature vector:\", loaded_data['pitch_features'][0])\n",
    "# print(\"First ZCR feature vector:\", loaded_data['zcr'][0])\n",
    "# print(\"First Amplitude Envelope feature vector:\", loaded_data['envelope'][0])\n",
    "# print(\"First HNR feature vector:\", loaded_data['hnr'][0])\n",
    "\n",
    "# Define feature combinations\n",
    "feature_combinations = {\n",
    "    'mfcc': mfcc_features,\n",
    "    'hist': hist_features,\n",
    "    'spectral_centroid': spectral_centroid_features,\n",
    "    'spectral_contrast': spectral_contrast_features,\n",
    "    'pitch': pitch_features,\n",
    "    'zcr': zcr_features,\n",
    "    'envelope': envelope_features,\n",
    "    'hnr': hnr_features\n",
    "}\n",
    "\n",
    "# Combine features based on flags\n",
    "combined_features = combine_features_with_flags(loaded_data, feature_selection)\n",
    "print(\"\\nCombined features shape:\", combined_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compare feature distributions for different classes\n",
    "\n",
    "# Convert combined_features to DataFrame\n",
    "df_features = pd.DataFrame(combined_features)\n",
    "df_features[\"label\"] = keys_list  # Add class labels\n",
    "\n",
    "# Ensure labels are categorical, not just numbers\n",
    "df_features[\"label\"] = df_features[\"label\"].astype(str)  # Convert to string labels\n",
    "\n",
    "# Select first 5 features for visualization (modify if needed)\n",
    "selected_features = df_features.iloc[:, -8:]  \n",
    "selected_features[\"label\"] = df_features[\"label\"]  # Re-add labels\n",
    "\n",
    "# Convert to long format for plotting\n",
    "df_melted = selected_features.melt(id_vars=\"label\", var_name=\"Feature\", value_name=\"Value\")\n",
    "\n",
    "# --- Strip Plot Fix ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.stripplot(x=\"Feature\", y=\"Value\", hue=\"label\", data=df_melted, dodge=True, jitter=True)\n",
    "plt.title(\"Feature Distributions with Strip Plot (Fixed Labels)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Class Labels\", bbox_to_anchor=(1.05, 1), loc='upper left')  # Fix legend\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Splitting\n",
    "X = combined_features\n",
    "y = np.array(keys_list)\n",
    "\n",
    "# Train-test split\n",
    "X_train_not_scaled, X_test_not_scaled, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=keys_list\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data using Z-score normalization (StandardScaler)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# feature normalization:\n",
    "# feature = feature - mean(feature acrocss all examples) / std(feature across all examples)\n",
    "## VERY IMPORTANT: the above mean and std must be calcualted from TRAIN data only, not from EVAL or TEST data\n",
    "\n",
    "# Fit only on the training data and transform both train and test sets\n",
    "X_train = scaler.fit_transform(X_train_not_scaled)  # Fit on train data only\n",
    "X_test = scaler.transform(X_test_not_scaled)        # Use train's mean and std to scale test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sizes of train and test sets\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)\n",
    "\n",
    "# Debugging - Check which classes exist before and after splitting\n",
    "unique_classes = set(keys_list)\n",
    "print(\"Unique classes in dataset before splitting:\", unique_classes)\n",
    "\n",
    "# Count occurrences of each class in train and test sets\n",
    "train_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "test_counts = pd.Series(y_test).value_counts().sort_index()\n",
    "\n",
    "# Print class counts for debugging purposes\n",
    "print(\"\\nTraining Class Counts:\\n\", train_counts)\n",
    "print(\"\\nTest Class Counts:\\n\", test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to Pandas Series for counting\n",
    "train_counts = pd.Series(y_train).value_counts()\n",
    "test_counts = pd.Series(y_test).value_counts()\n",
    "\n",
    "# Merge train and test counts into a single DataFrame\n",
    "df_counts = pd.DataFrame({'Train': train_counts, 'Test': test_counts}).fillna(0).astype(int)\n",
    "\n",
    "# Display the counts table with a better format\n",
    "print(\"\\nClass Distribution Table:\")\n",
    "print(df_counts.to_string())\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Total samples in Train set: {df_counts['Train'].sum()}\")\n",
    "print(f\"Total samples in Test set: {df_counts['Test'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_svm = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'gamma': [0.01, 0.1, 1, 'scale', 10, 100],\n",
    "    'kernel': ['rbf','linear']\n",
    "}\n",
    "\n",
    "grid_search_svm, best_params_svm, best_score_svm = grid_search_hyperparameter_tuning(\n",
    "    model=SVC(probability=True, random_state=42),\n",
    "    param_grid=param_grid_svm,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "cv_results = pd.DataFrame(grid_search_svm.cv_results_)\n",
    "\n",
    "# Convert parameters to numeric type for correct sorting\n",
    "cv_results[\"param_C\"] = cv_results[\"param_C\"].astype(float)\n",
    "cv_results[\"param_gamma\"] = cv_results[\"param_gamma\"].astype(str)  # Keep gamma as string if 'scale' is used\n",
    "\n",
    "# Use pivot_table instead of pivot to handle duplicate index-column pairs\n",
    "df_heatmap = cv_results.pivot_table(index=\"param_C\", columns=\"param_gamma\", values=\"mean_test_score\", aggfunc=\"mean\")\n",
    "\n",
    "# Sort the index for better visualization\n",
    "df_heatmap = df_heatmap.sort_index(ascending=True)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(df_heatmap, annot=True, cmap=\"coolwarm\", fmt=\".3f\")\n",
    "plt.title(\"AUC Scores for Different C and Gamma Values\")\n",
    "plt.xlabel(\"Gamma\")\n",
    "plt.ylabel(\"C\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SVM classifier with the best hyperparameters\n",
    "svm = SVC(kernel='rbf', C=10, gamma='scale', probability=True, random_state=42)\n",
    "\n",
    "# Train the SVM classifier on the training set\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "y_prob = svm.predict_proba(X_test)\n",
    "\n",
    "# Evaluate AUC on the test set\n",
    "test_auc = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
    "print(\"\\nAUC on the test set:\", test_auc)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy on the test set:\", accuracy)\n",
    "\n",
    "# Visualize the confusion matrix using heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_train), yticklabels=np.unique(y_train))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the labels for multi-class ROC (One-vs-Rest)\n",
    "y_test_binarized = label_binarize(y_test, classes=np.unique(y_test))\n",
    "\n",
    "# Initialize a plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Loop through each class and compute ROC curve and AUC\n",
    "for i in range(len(np.unique(y_test))):\n",
    "    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_prob[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'Class {np.unique(y_test)[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Plot settings\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)  # Diagonal line (random classifier)\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Multi-class ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce features to 30 dimensions using PCA\n",
    "pca = PCA(n_components=0.8)  \n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)  \n",
    "\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"Reduced feature shape:\", X_train_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='rbf', C=10, gamma='scale', probability=True, random_state=42)\n",
    "svm.fit(X_train_pca, y_train)\n",
    "\n",
    "# Step 3: Evaluate the classifier\n",
    "y_pred = svm.predict(X_test_pca)\n",
    "y_prob = svm.predict_proba(X_test_pca)\n",
    "\n",
    "# Step 4: Calculate metrics\n",
    "test_auc = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
    "print(\"\\nAUC on the test set:\", test_auc)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_train), yticklabels=np.unique(y_train))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_feature_combinations(\n",
    "    \n",
    "    combinations_dict=feature_combinations,\n",
    "    y=y,\n",
    "    model=SVC(),  # Specify the SVM model\n",
    "    model_name=\"SVM\",  # Provide a name for display pu,rposes\n",
    "    model_params={'kernel': 'rbf', 'C': 1, 'gamma': 'scale', 'probability': True, 'random_state': 42},  # Custom SVM parameters\n",
    "    n_splits=5,  # Number of Stratified K-Folds\n",
    "    normalize=True,  # Enable normalization of features\n",
    "    apply_pca=True,  # Apply PCA during feature processing\n",
    "    n_pca_components=0.9,  # Number of PCA components (explained variance ratio)\n",
    "    overfit_threshold=0.15,  # Overfitting threshold\n",
    "    use_gridsearch=False,  # Disable GridSearchCV for now\n",
    "    hyperparameter_grid={\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 0.1, 0.01],\n",
    "        'kernel': ['rbf']\n",
    "    } if False else None,  # Use direct Boolean instead of undefined variable\n",
    "    top_k_results=5,  # Display the top 5 combinations\n",
    "    verbose=False  # Enable detailed output for each combination\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_results_length_lim = [res for res in results if len(res['combination']) == 3 and res['overfitting_status'] == \"‚úÖ No Overfitting\"]\n",
    "top_5_results_length_lim.sort(key=lambda x: x['combined_score'], reverse=True)\n",
    "\n",
    "print(\"\\nTop 5 Non-Overfitted Combinations with Exactly 3 Features:\")\n",
    "for res in top_5_results_length_lim[:5]:  # Display the top 5\n",
    "    print(f\"Combination: {res['combination']}, AUC: {res['average_auc']:.4f}, \"\n",
    "          f\"Val Accuracy: {res['average_val_accuracy']:.4f}, Overfitting: {res['overfitting_status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information from top 5 non-overfitted results\n",
    "feature_names = [\"_\".join(res['combination']) for res in top_5_results_length_lim[:5]]\n",
    "accuracies = [res['average_val_accuracy'] for res in top_5_results_length_lim[:5]]\n",
    "auc_scores = [res['average_auc'] for res in top_5_results_length_lim[:5]]\n",
    "\n",
    "# Create the figure and axis\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Use different colors for bars\n",
    "colors = sns.color_palette(\"Blues\", len(feature_names))\n",
    "\n",
    "# Plot Accuracy (Bar chart)\n",
    "ax1.bar(feature_names, accuracies, color=colors, alpha=0.7, label='Accuracy')\n",
    "ax1.set_ylabel('Accuracy', color='b')\n",
    "ax1.set_ylim(0.5, 1.0)  # Set limits to fit the data well\n",
    "\n",
    "# Rotate X-axis labels for better readability\n",
    "plt.xticks(rotation=30, ha=\"right\", fontsize=10)\n",
    "\n",
    "# Add grid lines for better visual separation\n",
    "ax1.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Create a second y-axis for AUC scores\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot AUC as a red line with markers\n",
    "ax2.plot(feature_names, auc_scores, color='r', marker='o', linestyle='-', linewidth=2, markersize=8, label='AUC')\n",
    "ax2.set_ylabel('AUC Score', color='r')\n",
    "ax2.set_ylim(0.5, 1.0)  # Set limits to fit the data well\n",
    "\n",
    "# Formatting\n",
    "plt.title(\"SVM Performance on Top 5 Feature Combinations\", fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add legends\n",
    "ax1.legend(loc='upper left', fontsize=10)\n",
    "ax2.legend(loc='upper right', fontsize=10)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection = {\n",
    "    'mfcc': True,\n",
    "    'hist': True,\n",
    "    'spectral_centroid': True,\n",
    "    'spectral_contrast': True,\n",
    "    'pitch_features': True,\n",
    "    'zcr': True,\n",
    "    'envelope': True,\n",
    "    'hnr': True\n",
    "}\n",
    "\n",
    "selected_features, X_train, X_test, y_train, y_test, selected_features_names = load_and_split_features(\n",
    "    loaded_data, feature_selection\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results = kfold_cross_validation(\n",
    "    features=selected_features,\n",
    "    labels=y,\n",
    "    selected_features_names=selected_features_names,\n",
    "    model=SVC(probability=True),\n",
    "    model_params={'kernel': 'rbf', 'C': 1.0, 'gamma': 0.01, 'random_state': 42},\n",
    "    n_splits=5,\n",
    "    preprocess_params={'normalize': True, 'apply_pca': False, 'n_pca_components': 0.9},\n",
    "    overfit_threshold=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved features\n",
    "loaded_data = np.load(\"features/extracted_features_multiple_test.npz\")\n",
    "\n",
    "# Define which features to include (set True to include, False to exclude)\n",
    "feature_selection = {\n",
    "    'mfcc': True,\n",
    "    'hist': True,\n",
    "    'spectral_centroid': True,\n",
    "    'spectral_contrast': True,\n",
    "    'pitch_features': True,\n",
    "    'zcr': True,\n",
    "    'envelope': True,\n",
    "    'hnr': True\n",
    "}\n",
    "\n",
    "selected_features, X_train, X_test, y_train, y_test, selected_features_names = load_and_split_features(\n",
    "    loaded_data, feature_selection\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_prob_rf = rf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC score\n",
    "test_auc_rf = roc_auc_score(y_test, y_prob_rf, multi_class='ovr')\n",
    "\n",
    "# Accuracy\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nRandom Forest - AUC on the test set:\", test_auc_rf)\n",
    "print(\"\\nRandom Forest - Accuracy on the test set:\", accuracy_rf)\n",
    "y_train_pred = rf.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"\\nTraining Accuracy:\", train_accuracy)\n",
    "print(\"\\nRandom Forest - Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Confusion matrix visualization\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_train), yticklabels=np.unique(y_train))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Random Forest - Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = rf.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"\\nTraining Accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=5, scoring='roc_auc_ovr', n_jobs=-1, verbose=1)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "print(\"\\nBest Parameters for Random Forest:\", grid_search_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=10, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_prob_rf = rf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_auc_rf = roc_auc_score(y_test, y_prob_rf, multi_class='ovr')\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nRandom Forest - AUC on the test set:\", test_auc_rf)\n",
    "print(\"\\nRandom Forest - Accuracy on the test set:\", accuracy_rf)\n",
    "y_train_pred = rf.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"\\nTraining Accuracy:\", train_accuracy)\n",
    "print(\"\\nRandom Forest - Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Confusion matrix visualization\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_train), yticklabels=np.unique(y_train))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Random Forest - Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = test_feature_combinations(\n",
    "    combinations_dict=feature_combinations,  # Your feature dictionary\n",
    "    y=y,  # Your corresponding labels\n",
    "    model=RandomForestClassifier(),  # RF model\n",
    "    model_name=\"Random Forest\",  # Model name for display purposes\n",
    "    model_params={  # Default RF parameters (if grid search is off)\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 10,\n",
    "        'min_samples_split': 5,\n",
    "        'random_state': 42\n",
    "    },\n",
    "    n_splits=5,  # Number of Stratified K-Folds\n",
    "    normalize=False,  # Disable normalization of features\n",
    "    apply_pca=False,  # Disable PCA here for testing; set to True if needed\n",
    "    n_pca_components=10,  # Number of PCA components to apply if needed\n",
    "    overfit_threshold=0.2,  # Threshold to detect overfitting\n",
    "    use_gridsearch=False,  # Disable or enable GridSearch\n",
    "    hyperparameter_grid={  # Grid is always passed, but used only when use_gridsearch=True\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    top_k_results=5,  # Display top 5 combinations\n",
    "    verbose=True  # Enable detailed output for each combination\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection = {\n",
    "    'mfcc': False,\n",
    "    'hist': False,\n",
    "    'spectral_centroid': True,\n",
    "    'spectral_contrast': True,\n",
    "    'pitch_features': True,\n",
    "    'zcr': False,\n",
    "    'envelope': False,\n",
    "    'hnr': False\n",
    "}\n",
    "selected_features, X_train, X_test, y_train, y_test, selected_features_names = load_and_split_features(\n",
    "    loaded_data, feature_selection\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = kfold_cross_validation(\n",
    "    features=selected_features,\n",
    "    labels=y,\n",
    "    selected_features_names=selected_features_names,\n",
    "    model=RandomForestClassifier(),\n",
    "    model_params={\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 10,\n",
    "        'min_samples_split': 5,\n",
    "        'random_state': 42\n",
    "    },\n",
    "    n_splits=5,\n",
    "    preprocess_params={'normalize': False, 'apply_pca': False, 'n_pca_components': 10},\n",
    "    overfit_threshold=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved features\n",
    "loaded_data = np.load(\"features/extracted_features_multiple_test.npz\")\n",
    "\n",
    "# Define which features to include (set True to include, False to exclude)\n",
    "feature_selection = {\n",
    "    'mfcc': True,\n",
    "    'hist': True,\n",
    "    'spectral_centroid': True,\n",
    "    'spectral_contrast': True,\n",
    "    'pitch_features': True,\n",
    "    'zcr': True,\n",
    "    'envelope': True,\n",
    "    'hnr': True\n",
    "}\n",
    "\n",
    "selected_features, X_train, X_test, y_train, y_test, selected_features_names = load_and_split_features(\n",
    "    loaded_data, feature_selection\n",
    ")\n",
    "\n",
    "X_train, X_test = preprocess_features(\n",
    "    X_train, X_test, normalize=True, apply_pca=False, n_pca_components=0.9, verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN classifier with k=3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred = knn.predict(X_test)\n",
    "y_prob = knn.predict_proba(X_test)\n",
    "\n",
    "# Evaluation\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_auc = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
    "\n",
    "# Predictions on train set for training accuracy\n",
    "y_train_pred = knn.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"\\nKNN Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"KNN Test AUC: {test_auc:.4f}\")\n",
    "print(f\"\\nKNN Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('KNN Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "grid_search_knn, best_params_knn, best_score_knn = grid_search_hyperparameter_tuning(\n",
    "    model=KNeighborsClassifier(),\n",
    "    param_grid=param_grid_knn,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_feature_combinations(\n",
    "    combinations_dict,\n",
    "    y,\n",
    "    model,\n",
    "    model_name,\n",
    "    model_params=None,\n",
    "    n_splits=5,\n",
    "    normalize=True,\n",
    "    apply_pca=True,\n",
    "    n_pca_components=0.9,\n",
    "    overfit_threshold=0.1,\n",
    "    hyperparameter_grid=None,\n",
    "    use_gridsearch=False,\n",
    "    top_k_results=5,\n",
    "    verbose=False\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    # Generate all possible non-empty combinations of features\n",
    "    all_combinations = [comb for i in range(1, len(combinations_dict) + 1) \n",
    "                        for comb in itertools.combinations(combinations_dict.keys(), i)]\n",
    "\n",
    "    print(f\"\\nüîç Testing {len(all_combinations)} feature combinations for {model_name}...\\n\")\n",
    "\n",
    "    for combination in all_combinations:\n",
    "        # Select features\n",
    "        selected_features = np.hstack([combinations_dict[feature] for feature in combination])\n",
    "\n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            selected_features, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "\n",
    "        # Preprocess features (Normalize, PCA if applicable)\n",
    "        X_train, X_test = preprocess_features(\n",
    "            X_train, X_test, \n",
    "            normalize=normalize, \n",
    "            apply_pca=apply_pca, \n",
    "            n_pca_components=n_pca_components,\n",
    "            verbose=False  # Silence preprocessing output during the loop\n",
    "        )\n",
    "\n",
    "        # Optionally perform GridSearch\n",
    "        if use_gridsearch:\n",
    "            if hyperparameter_grid:\n",
    "                print(f\"\\nPerforming grid search for combination: {combination}...\")\n",
    "                grid_search, best_params, best_score = grid_search_hyperparameter_tuning(\n",
    "                    model=model,\n",
    "                    param_grid=hyperparameter_grid,\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train\n",
    "                )\n",
    "                model_params = best_params\n",
    "                print(f\"Grid search complete. Best parameters: {best_params}, Best AUC: {best_score:.4f}\")\n",
    "            else:\n",
    "                raise ValueError(\"Grid search enabled, but no hyperparameter grid provided.\")\n",
    "        elif not model_params:\n",
    "            raise ValueError(\"Provide `model_params` or enable grid search.\")\n",
    "\n",
    "        # Cross-validation using K-Fold\n",
    "        train_accuracies, val_accuracies, auc_scores = [], [], []\n",
    "        kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "        for train_idx, val_idx in kfold.split(X_train, y_train):\n",
    "            fold_X_train, fold_X_val = X_train[train_idx], X_train[val_idx]\n",
    "            fold_y_train, fold_y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "            # Initialize and train the model\n",
    "            classifier = model.set_params(**model_params)\n",
    "            classifier.fit(fold_X_train, fold_y_train)\n",
    "\n",
    "            # Training and validation accuracy\n",
    "            train_acc = accuracy_score(fold_y_train, classifier.predict(fold_X_train))\n",
    "            val_acc = accuracy_score(fold_y_val, classifier.predict(fold_X_val))\n",
    "            train_accuracies.append(train_acc)\n",
    "            val_accuracies.append(val_acc)\n",
    "\n",
    "            # Calculate AUC if available\n",
    "            if hasattr(classifier, \"predict_proba\"):\n",
    "                y_val_proba = classifier.predict_proba(fold_X_val)\n",
    "                auc_scores.append(roc_auc_score(fold_y_val, y_val_proba, multi_class='ovr'))\n",
    "\n",
    "        # Calculate averages\n",
    "        avg_train_accuracy = np.mean(train_accuracies)\n",
    "        avg_val_accuracy = np.mean(val_accuracies)\n",
    "        avg_auc = np.mean(auc_scores) if auc_scores else None\n",
    "        overfitting_gap = avg_train_accuracy - avg_val_accuracy\n",
    "        overfitting_status = \"‚ö†Ô∏è Overfitting Risk\" if overfitting_gap > overfit_threshold else \"‚úÖ No Overfitting\"\n",
    "\n",
    "        # Calculate combined score: 50% AUC + 50% validation accuracy\n",
    "        combined_score = 0.5 * (avg_auc if avg_auc is not None else 0) + 0.5 * avg_val_accuracy\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            'combination': combination,\n",
    "            'average_auc': avg_auc,\n",
    "            'average_train_accuracy': avg_train_accuracy,\n",
    "            'average_val_accuracy': avg_val_accuracy,\n",
    "            'combined_score': combined_score,\n",
    "            'overfitting_status': overfitting_status,\n",
    "            'overfitting_gap': overfitting_gap\n",
    "        })\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Combination: {combination}, \"\n",
    "                  f\"AUC: {f'{avg_auc:.4f}' if avg_auc is not None else 'N/A'}, \"\n",
    "                  f\"Train Acc: {avg_train_accuracy:.4f}, \"\n",
    "                  f\"Val Acc: {avg_val_accuracy:.4f}, \"\n",
    "                  f\"Overfitting: {overfitting_status}\")\n",
    "\n",
    "    # Filter and sort by combined score\n",
    "    sorted_results = sorted(results, key=lambda x: x['combined_score'], reverse=True)\n",
    "\n",
    "    # Display top-k results\n",
    "    print(f\"\\nüìä Top {top_k_results} Results (sorted by Combined Score):\")\n",
    "    for i, res in enumerate(sorted_results[:top_k_results], start=1):\n",
    "        print(f\"  {i}. Combination: {res['combination']}, \"\n",
    "              f\"Combined Score: {res['combined_score']:.4f}, \"\n",
    "              f\"Test Acc: {res['average_val_accuracy']:.4f}, \"\n",
    "              f\"Train Acc: {res['average_train_accuracy']:.4f}, \"\n",
    "              f\"AUC: {f'{res['average_auc']:.4f}' if res['average_auc'] is not None else 'N/A'}, \"\n",
    "              f\"Train-Test Gap: {res['overfitting_gap']:.4f}, \"\n",
    "              f\"Overfitting Status: {res['overfitting_status']}\")\n",
    "\n",
    "    return sorted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_results = test_feature_combinations(\n",
    "    combinations_dict=feature_combinations,  # Your feature dictionary\n",
    "    y=y,  # Your corresponding labels\n",
    "    model=KNeighborsClassifier(),  # KNN model\n",
    "    model_name=\"KNN\",  # Model name for display purposes\n",
    "    model_params={\n",
    "        'n_neighbors': 7,\n",
    "        'weights': 'uniform',\n",
    "        'metric': 'euclidean'\n",
    "    },  # KNN parameters\n",
    "    n_splits=5,  # Number of Stratified K-Folds\n",
    "    normalize=True,  # Enable normalization of features\n",
    "    apply_pca=False,  # Disable PCA for now\n",
    "    n_pca_components=10,  # Number of PCA components if apply_pca=True\n",
    "    overfit_threshold=0.1,  # Threshold to detect overfitting\n",
    "    use_gridsearch=False,  # Disable grid search in this example\n",
    "    top_k_results=5,  # Show top 5 combinations\n",
    "    verbose=True  # printing all combinations during the loop\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection = {\n",
    "    'mfcc': True,\n",
    "    'hist': False,\n",
    "    'spectral_centroid': True,\n",
    "    'spectral_contrast': True,\n",
    "    'pitch_features': True,\n",
    "    'zcr': False,\n",
    "    'envelope': True,\n",
    "    'hnr': True\n",
    "}\n",
    "\n",
    "selected_features, X_train, X_test, y_train, y_test, selected_features_names = load_and_split_features(\n",
    "    loaded_data, feature_selection\n",
    ")\n",
    "\n",
    "X_train, X_test = preprocess_features(\n",
    "    X_train, X_test, normalize=True, apply_pca=False, n_pca_components=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7, weights='uniform', metric='euclidean')\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred = knn.predict(X_test)\n",
    "y_prob = knn.predict_proba(X_test)\n",
    "\n",
    "# Evaluation\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_auc = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
    "\n",
    "# Predictions on train set for training accuracy\n",
    "y_train_pred = knn.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"\\nKNN Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"KNN Test AUC: {test_auc:.4f}\")\n",
    "print(f\"\\nKNN Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('KNN Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection = {\n",
    "    'mfcc': True,\n",
    "    'hist': False,\n",
    "    'spectral_centroid': True,\n",
    "    'spectral_contrast': True,\n",
    "    'pitch_features': True,\n",
    "    'zcr': False,\n",
    "    'envelope': True,\n",
    "    'hnr': True\n",
    "}\n",
    "\n",
    "# selected_features, selected_features_names = get_selected_features(feature_combinations, feature_selection)\n",
    "\n",
    "selected_features, X_train, X_test, y_train, y_test, selected_features_names = load_and_split_features(\n",
    "    loaded_data, feature_selection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_results = kfold_cross_validation(\n",
    "    features=selected_features,\n",
    "    labels=y,\n",
    "    selected_features_names=selected_features_names,\n",
    "    model=KNeighborsClassifier(),  # Initialize the KNN model\n",
    "    model_params={\n",
    "        'n_neighbors': 7,\n",
    "        'weights': 'uniform',\n",
    "        'metric': 'euclidean'\n",
    "    },\n",
    "    n_splits=5,\n",
    "    preprocess_params={'normalize': True, 'apply_pca': False, 'n_pca_components': 0.8},\n",
    "    overfit_threshold=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "# 1Ô∏è‚É£ Step 1: Feature Selection\n",
    "feature_selection = {\n",
    "    'mfcc': True,\n",
    "    'hist': False,\n",
    "    'spectral_centroid': True,\n",
    "    'spectral_contrast': True,\n",
    "    'pitch_features': True,\n",
    "    'zcr': True,\n",
    "    'envelope': True,\n",
    "    'hnr': True\n",
    "}\n",
    "\n",
    "# Load selected features\n",
    "selected_features, X_train, X_test, y_train, y_test, selected_features_names = load_and_split_features(\n",
    "    loaded_data, feature_selection\n",
    ")\n",
    "\n",
    "# 2Ô∏è‚É£ Step 2: Normalize the Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)  # Use the same scaler for the test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£ Step 3: Apply PCA for Dimensionality Reduction\n",
    "pca = PCA(n_components=0.95)  # Keep 95% variance\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "print(\"Number of PCA Components Used:\", pca.n_components_)\n",
    "print(\"Explained Variance:\", np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "# 4Ô∏è‚É£ Step 4: Find the Optimal Number of Clusters using the Elbow Method\n",
    "inertia = []\n",
    "sil_scores = []  # ‚úÖ FIXED: Define silhouette scores list\n",
    "\n",
    "K_range = range(2, 20)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=20)\n",
    "    labels = kmeans.fit_predict(X_train_pca)\n",
    "    \n",
    "    inertia.append(kmeans.inertia_)\n",
    "    sil_scores.append(silhouette_score(X_train_pca, labels))  # ‚úÖ FIXED: Compute Silhouette Score\n",
    "\n",
    "# ‚úÖ Plot Elbow Curve\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(K_range, inertia, marker='o')\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.title(\"Elbow Method for Optimal k\")\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ Plot Silhouette Scores\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(K_range, sil_scores, marker='o', label='Silhouette Score', color='red')\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.title(\"Silhouette Score for Different k Values\")\n",
    "plt.show()\n",
    "\n",
    "# 5Ô∏è‚É£ Step 5: Perform K-Means Clustering with k=10 (Known Value)\n",
    "best_k = 10  # Since we already know the correct number of clusters\n",
    "\n",
    "#kmeans = KMeans(n_clusters=best_k, init='k-means++', random_state=42, n_init=20, max_iter=5000) \n",
    "kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=30)\n",
    "cluster_labels = kmeans.fit_predict(X_train_pca)\n",
    "\n",
    "# 6Ô∏è‚É£ Step 6: Evaluate Clustering Performance using Silhouette Score\n",
    "silhouette = silhouette_score(X_train_pca, cluster_labels)\n",
    "print(f\"Silhouette Score for k=10: {silhouette:.4f}\")\n",
    "\n",
    "# 7Ô∏è‚É£ Step 7: Visualize Clusters using t-SNE\n",
    "X_embedded = TSNE(n_components=2, perplexity=15, random_state=42).fit_transform(X_train_pca)  # ‚úÖ FIXED Perplexity\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=cluster_labels, cmap='tab10', alpha=0.7)\n",
    "plt.colorbar()\n",
    "plt.title(\"t-SNE Visualization of Clusters\")\n",
    "plt.show()\n",
    "\n",
    "# 8Ô∏è‚É£ Step 8: Evaluate K-Means with Adjusted Rand Index (ARI)\n",
    "ari_score = adjusted_rand_score(y_train, cluster_labels)\n",
    "print(f\"Adjusted Rand Index (ARI) for K-Means: {ari_score:.4f}\")\n",
    "\n",
    "\n",
    "# üîü Step 9: Assign Labels to Clusters using Majority Voting\n",
    "def assign_labels_majority(y_true, cluster_labels):\n",
    "    assigned_labels = np.zeros_like(cluster_labels)\n",
    "\n",
    "    for cluster in np.unique(cluster_labels):\n",
    "        mask = (cluster_labels == cluster)\n",
    "        true_labels = y_true[mask]  # Get true labels in this cluster\n",
    "        \n",
    "        if len(true_labels) == 0:  # Edge case: No data points in cluster\n",
    "            continue \n",
    "        \n",
    "        if len(true_labels) > 0:\n",
    "            majority_label = mode(true_labels, keepdims=True).mode[0]\n",
    "        else:\n",
    "            majority_label = -1  # Assign an unused label if cluster is empty\n",
    "\n",
    "        \n",
    "        assigned_labels[mask] = majority_label  # Assign majority label to all cluster members\n",
    "\n",
    "    return assigned_labels\n",
    "\n",
    "y_pred = assign_labels_majority(y_train, cluster_labels)\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ Step 11: Confusion Matrix\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Cluster')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix: True Labels vs. Cluster Assignments')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(cluster_labels, return_counts=True)\n",
    "cluster_distribution = dict(zip(unique, counts))\n",
    "\n",
    "print(\"\\nüîç Cluster Size Distribution:\")\n",
    "for cluster, count in sorted(cluster_distribution.items()):\n",
    "    print(f\"Cluster {cluster}: {count} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in np.unique(cluster_labels):\n",
    "    assigned_class = mode(y_train[cluster_labels == cluster], keepdims=True).mode[0]\n",
    "    print(f\"Cluster {cluster} is mostly class {assigned_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_class_df = pd.DataFrame({'Cluster': cluster_labels, 'True Class': y_train})\n",
    "cluster_counts = cluster_class_df.groupby(['Cluster', 'True Class']).size().unstack(fill_value=0)\n",
    "\n",
    "cluster_counts.plot(kind='bar', stacked=True, figsize=(10,6), colormap='tab10')\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.title(\"Class Distribution in Each Cluster\")\n",
    "plt.legend(title=\"True Class\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
